{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from functools import reduce, lru_cache\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "def data_extend(data, k):\n",
    "    r, c = data.shape\n",
    "    data_extend = (reduce(lambda x, y: np.vstack((x, y)),\n",
    "                          map(lambda x: data, range(k))) +\n",
    "                  np.random.randn(r*c*k).reshape(r*k, c).round(1))\n",
    "    return(data_extend)\n",
    "\n",
    "def hac(k, data):\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # start with each point in its own cluster\n",
    "    clusters = np.arange(n)\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    while unique_clusters > k:\n",
    "        min_distances = np.zeros(n)\n",
    "        min_points = np.zeros(n).astype('int')\n",
    "\n",
    "        # for each point, find min distance to point not in cluster\n",
    "        for i in range(n):\n",
    "            point = data[i,]\n",
    "            point_cluster = clusters[i]\n",
    "            distances = np.linalg.norm(point - data, axis = 1)\n",
    "            diff_cluster_points = np.where(\n",
    "                clusters != point_cluster)[0]\n",
    "\n",
    "            min_points[i] = diff_cluster_points[\n",
    "                np.argmin(distances[diff_cluster_points])]\n",
    "            min_distances[i] = distances[min_points[i]]\n",
    "\n",
    "        # merge clusters of the two closest points\n",
    "        point1_idx = np.argmin(min_distances)\n",
    "        point1 = data[point1_idx,]\n",
    "        point2_idx = min_points[point1_idx]\n",
    "        point2 = data[point2_idx,]\n",
    "\n",
    "        point2_cluster = clusters[point2_idx]\n",
    "        clusters[np.where(\n",
    "            clusters == point2_cluster\n",
    "        )[0]] = clusters[point1_idx]\n",
    "\n",
    "        # update number of clusters\n",
    "        unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def unary(x, C):\n",
    "    nearest_x = int(np.round(x))\n",
    "    return(np.r_[np.ones(nearest_x), np.zeros(C-nearest_x)])\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def lsh_hash(point, C):\n",
    "    point = np.array(point)\n",
    "    res = np.concatenate(list(map(lambda x: unary(x, C), point)))\n",
    "    return(res)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_points_in_cluster(idx, clusters, data):\n",
    "    clusters = np.array(clusters)\n",
    "    data = pickle.loads(data)\n",
    "    point_cluster = clusters[idx]\n",
    "    same_cluster_points_idx = np.where(\n",
    "        clusters == point_cluster)[0]\n",
    "    same_cluster_points = set(\n",
    "        map(tuple, data[same_cluster_points_idx, :])\n",
    "    )\n",
    "    return(same_cluster_points)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_point_indices(data, points):\n",
    "    data = pickle.loads(data)\n",
    "    points = pickle.loads(points)\n",
    "    indices = np.where((data == points[:,None]).all(-1))[1]\n",
    "    return(indices)\n",
    "\n",
    "def clear_caches():\n",
    "    unary.cache_clear()\n",
    "    lsh_hash.cache_clear()\n",
    "    get_points_in_cluster.cache_clear()\n",
    "    get_point_indices.cache_clear()\n",
    "\n",
    "def build_hash_tables(C, d, l, k, data, clusters):\n",
    "    vals = np.arange(C*d)\n",
    "    n = data.shape[0]\n",
    "    hash_tables = defaultdict(set)\n",
    "    hash_tables_reversed = defaultdict(set)\n",
    "\n",
    "    for i in range(l):\n",
    "        I = np.random.choice(vals, k, replace = False)\n",
    "\n",
    "        for j in range(n):\n",
    "            # for every point, generate hashed point\n",
    "            # and sample k bits\n",
    "            p = data[j]\n",
    "            hashed_point = lsh_hash(tuple(p), C)[I]\n",
    "            \n",
    "            # check if any other points in p's cluster are\n",
    "            # already in this hash table and only add point to\n",
    "            # hash table if no other points from its cluster are there\n",
    "            bucket = hash_tables[tuple(hashed_point)]\n",
    "            cluster_points = get_points_in_cluster(\n",
    "                j, tuple(clusters), pickle.dumps(data)\n",
    "            )\n",
    "            \n",
    "            # create unique bucket for each hash function\n",
    "            #key = tuple([i]) + tuple(hashed_point)\n",
    "\n",
    "            if not cluster_points.intersection(bucket):\n",
    "                hash_tables[tuple(hashed_point)].add(tuple(p))\n",
    "                hash_tables_reversed[tuple(p)].add(tuple(hashed_point))\n",
    "    \n",
    "    return hash_tables, hash_tables_reversed\n",
    "\n",
    "def LSHLink(data, A, l, k, C = None, cutoff = 1, dendrogram = False, **kwargs):\n",
    "    # set default value for C if none is provided\n",
    "    if not C:\n",
    "        C = int(np.ceil(np.max(data))) + 1\n",
    "    \n",
    "    if dendrogram and cutoff != 1:\n",
    "        raise Exception(\n",
    "            'Dendrogram requires a full hierarchy; set cutoff to 1'\n",
    "        )\n",
    "    \n",
    "    # initializations\n",
    "    n, d = data.shape\n",
    "    clusters = np.arange(n)\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    num = n - 1\n",
    "    Z = np.zeros((n - 1, 4))\n",
    "    \n",
    "    # calculate r depending on n, either:\n",
    "    # 1. min dist from a random sample of sqrt(n) points\n",
    "    # 2. formula below\n",
    "    \n",
    "#     if 'seed1' in kwargs and isinstance(kwargs['seed1'], int):\n",
    "#         np.random.seed(kwargs['seed1'])\n",
    "    np.random.seed(12)\n",
    "        \n",
    "    n_samp = int(np.ceil(np.sqrt(n)))\n",
    "    samples = data[\n",
    "        np.random.choice(n, size = n_samp, replace = False), :]\n",
    "    \n",
    "    if n < 500:\n",
    "        r = np.min(pdist(samples, 'euclidean'))\n",
    "    else:\n",
    "        r = (d * C * np.sqrt(d)) / (2 * (k + d))\n",
    "    \n",
    "#     if 'seed2' in kwargs and isinstance(kwargs['seed2'], int):\n",
    "#         np.random.seed([kwargs['seed2']])\n",
    "    np.random.seed(6)\n",
    "\n",
    "    while unique_clusters > cutoff:\n",
    "        # STEP 1: Generation of hash tables\n",
    "        hash_tables, hash_tables_reversed = build_hash_tables(\n",
    "            C, d, l, k, data, clusters)\n",
    "\n",
    "        # STEP 2: Nearest neighbor search for p\n",
    "        for i in range(n):\n",
    "            # get all of those hash tables that contain point p\n",
    "            p = data[i]\n",
    "            p_hashes = hash_tables_reversed[tuple(p)]\n",
    "\n",
    "            # only proceed if p is in at least one hash table\n",
    "            if hash_tables_reversed[tuple(p)]:\n",
    "\n",
    "                # find all \"similar points\" to p: points that share\n",
    "                # at least one hash table with p, and are not in the\n",
    "                # same cluster as p\n",
    "                similar_points = reduce(\n",
    "                    lambda x, y: x.union(y),\n",
    "                    map(lambda x: hash_tables[x], p_hashes)\n",
    "                    ).difference(\n",
    "                    get_points_in_cluster(i, tuple(clusters),\n",
    "                                          pickle.dumps(data))\n",
    "                )\n",
    "                similar_points = np.array(list(similar_points))\n",
    "\n",
    "                # STEP 3: Connect pairs of clusters within certain\n",
    "                # distance of p\n",
    "                # only proceed if p has any similar points\n",
    "                if similar_points.size:\n",
    "\n",
    "                    # find similar points q s.t. dist(p, q) < r\n",
    "                    # the clusters containing these points will be\n",
    "                    # merged with p's cluster\n",
    "                    points_to_merge = similar_points[\n",
    "                        np.where(\n",
    "                            np.linalg.norm(p - similar_points, axis = 1) < r\n",
    "                        )[0]\n",
    "                    ]\n",
    "\n",
    "                    # only proceed if p has similar points within distance r\n",
    "                    if points_to_merge.size:\n",
    "                        # identify which clusters contain points_to_merge\n",
    "                        point_indices = get_point_indices(\n",
    "                            pickle.dumps(data),\n",
    "                            pickle.dumps(points_to_merge)\n",
    "                        )\n",
    "                        clusters_to_merge = list(\n",
    "                            np.unique(clusters[point_indices])\n",
    "                        )\n",
    "                        \n",
    "                        # update cluster labels\n",
    "                        # if dendrogram = False, we can use a simpler method\n",
    "                        if not dendrogram:\n",
    "                            clusters[\n",
    "                                np.where(np.in1d(clusters, clusters_to_merge)\n",
    "                                        )[0]] = clusters[i]\n",
    "                        \n",
    "                        else:\n",
    "                            clusters_to_merge.append(clusters[i])\n",
    "                            \n",
    "                            for j in range(len(clusters_to_merge) - 1):\n",
    "                                clusterA = clusters_to_merge[j]\n",
    "                                clusterB = clusters_to_merge[j+1]\n",
    "                                num += 1\n",
    "                                clusters[np.where(\n",
    "                                    np.in1d(clusters, [clusterA, clusterB])\n",
    "                                )[0]] = num\n",
    "\n",
    "                                Z[num - n, :] = np.array(\n",
    "                                    [clusterA,\n",
    "                                     clusterB,\n",
    "                                     r,\n",
    "                                     len(np.where(np.in1d(clusters, num))[0])]\n",
    "                                )\n",
    "                                clusters_to_merge[j:j+2] = 2 * [num]\n",
    "\n",
    "        # STEP 4: update parameters and continue until\n",
    "        # unique_clusters == cutoff\n",
    "        unique_clusters = len(np.unique(clusters))\n",
    "\n",
    "        #increase r and decrease k\n",
    "        r *= A\n",
    "        k = int(np.round((d * C * np.sqrt(d)) / (2 * r)))\n",
    "\n",
    "    if not dendrogram:\n",
    "        return(clusters)\n",
    "    \n",
    "    else:\n",
    "        return(clusters, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris().data\n",
    "iris = data_extend(iris, 20) * 10\n",
    "iris += np.abs(np.min(iris))\n",
    "\n",
    "l = 10\n",
    "k = 100\n",
    "n, d = iris.shape\n",
    "C = int(np.ceil(np.max(iris))) + 1\n",
    "clusters = np.arange(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 150\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.75 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 450\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 1050\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 1500\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 1950\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 2550\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 2550?\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 3000\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 150\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 450\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 1050\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 1500\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 1950\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 2550\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 34s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n = 3000\n",
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_A14_y = [.46, 2.37, 12.4, 20, 37.2, 56.4, 94]\n",
    "lsh_A14_x = [150, 450, 1050, 1500, 1950, 2550, 3000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
