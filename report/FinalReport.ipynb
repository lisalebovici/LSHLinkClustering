{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of Fast Agglomerative Hierarchical Clustering Algorithm Using Locality-Sensitive Hashing by Walker Harrison and Lisa Lebovici (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Agglomerative hierarchical clustering is a widely used clustering algorithm in which n data points are sequentially merged according to some distance metric from n clusters into a single cluster. While many different metrics exist to evaluate inter-cluster distance, single-linkage reigns among the most popular; according to this method, pairwise distances are computed between all points that do not share a cluster, and the clusters containing the two points with the shortest euclidean distance are joined in each iteration. However, single-linkage has some drawbacks, most noteworthy that it scales quadratically as data grows. As such, Koga, Ishibashi, and Watanbe propose using locality-sensitive hashing (hereafter LSH) as an approximation to the single-linkage method. Under LSH, points are hashed into buckets such that the distance from a given point p need only be computed to the subset of points with which it shares a bucket.\n",
    "\n",
    "In this paper, we implement locality-sensitive hashing as a linkage method for agglomerative hierarchical clustering, optimize the code with an emphasis on reducing the time complexity of the creation of the hash tables, and apply the algorithm to a variety of synthetic and real-world data sets.\n",
    "\n",
    "**Keywords**: agglomerative hierarchical clustering, locality-sensitive hashing, single-linkage, nearest neighbor search, dendrogram similarity, cophenetic correlation coefficient\n",
    "\n",
    "**Github link**: https://github.com/lisalebovici/LSHLinkClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "With the growing ease in which data can be acquired in the modern age, techniques that can accurately represent this data are becoming increasingly important. In particular, unsupervised learning - that is, data which has no \"ground truth\" representation - finds itself at the center of many fields ranging from consumer marketing to computer vision to genetics. At its heart, the goal of unsupervised learning is pattern recognition. To this end, algorithms such as k-means and agglomerative hierarchical clustering have been sufficiently effective until now; but as the size of data continues to grow, scalability issues are becoming a bigger barrier to analysis and understanding.\n",
    "\n",
    "Koga et al.'s *Fast Agglomerative Hierarchical Clustering Algorithm Using Locality-Sensitive Hashing* provides a faster approximation to single-linkage hierarchical agglomerative clustering for large data, which has a run time complexity of $O(n^2)$. The single-linkage method requires that the distances from every point to all other points not in its cluster are calculated on every iteration (of which there are $n-1$ iterations in a size $n$ data set), which becomes prohibitively expensive. In contrast, the primary advantage of LSH is that it reduces the number of distances that need to be computed as well as the the number of iterations that need to run, resulting in a linear run time complexity $O(nB)$ (where $B$ is the maximum number of points in a single hash table).\n",
    "\n",
    "However, one consequence of the gain in run time is that LSH results in a coarser approximation of the data than single-linkage; while single-linkage necessarily creates clusters ranging from size $n$ to size $1$, LSH yields far fewer iterations of clusters. This granularity can be controlled by a parameter within the algorithm, but in general it is not expected to reproduce the single-linkage method exactly. Nonetheless, when granularity is not a primary concern, the improvements in terms of efficiency make it a worthwhile alternative.\n",
    "\n",
    "We will proceed by describing the implementation of LSH for hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Algorithm\n",
    "\n",
    "Some important notation will first be defined:\n",
    "\n",
    "- $n$: number of rows in data\n",
    "- $d$: dimension of data\n",
    "- $C$: least integer greater than the maximal coordinate value in the data\n",
    "- $\\ell$: number of hash functions\n",
    "- $k$: number of sampled bits from a hashed value\n",
    "- $r$: minimal distance between points required to merge clusters\n",
    "- $A$: increase ratio of r on each iteration\n",
    "\n",
    "LSH works in two primary phases: first, by creating a series of hash tables in which the data points are placed, and second, by computing the distances between a point and its \"similar\" points to determine which clusters should be merged.\n",
    "\n",
    "**Phase 1: Generation of hash tables**. Suppose we have a d-dimensional point $x$. A unary function is applied to each coordinate value of $x$ such that $x$ 1s are followed by $C-x$ 0s. The sequence of 1s and 0s for all coordinate values of $x$ are then joined to form the hashed point of $x$. Some number $k$ bits are then sampled without replacement from the hashed point. For example, if $x$ is the point $(2, 1, 3)$ and $C = 4$, the hashed point would be $\\underbrace{1100}_{2}\\underbrace{1000}_{1}\\underbrace{1110}_{3}$. If $k = 5$ and the indices $I = {5, 9, 2, 11, 8}$ are randomly sampled, then our resulting value would be $11110$. $x$ would thus be placed into the hash table for $11110$.\n",
    "\n",
    "This hash function is applied to all points, and a point is added to the corresponding hash table if no other point in its cluster is already present (that is to say, hash tables, are uniqued by cluster). Another set of $k$ indices $I$ is then randomly sampled and the resulting hash function is applied to all of the points. This procedure is repeated $\\ell$ times.\n",
    "\n",
    "The intuition behind this step is as follows: a point $s$ that is very similar to $x$ is likely to have a similar hash sequence to $x$ and therefore appear in many of the same hash tables as $x$. However, for any given hash, there is no guarantee; for example, for $s = (1, 1, 3)$, the hashed point would be $\\underbrace{1000}_{1}\\underbrace{1000}_{1}\\underbrace{1110}_{3}$ and the sampled value would be $11010$. In this cae, $s$ would not be in the same value as $x$ above. However, by applying $\\ell$ hash functions to the data, we have some certainty that if $s$ is really close to $x$, it will share at least one hash table.\n",
    "\n",
    "**Phase 2: Nearest neighbor search**. For each point $x$, we find all of the points that share at least one hash table with $x$ and are not currently in the same cluster as $x$. These are the similar points which are candidates to have their clusters merged with $x$'s cluster. The distances between $x$ and the similar points are computed, and for all points $p$ for which the euclidean distance between $x$ and $p$ is less than $r$, $p$'s clusters are merged with $x$'s cluster.\n",
    "\n",
    "If there is more than one cluster remaining after the merges, the values for $r$ and $k$ are updated and then phases 1 and 2 are repeated. $r$ is increased (we now consider points that are a slightly further distance away than previously to merge further-away clusters) and $k$ is decreased (so that the hashed values become shorter and therefore more common). This continues until all points are in the same cluster, at which point the algorithm terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH-Link Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Input:** Starting values for $\\ell$, $k$, and $A$\n",
    "\n",
    "> **Initialize**:\n",
    "\n",
    "> > **if** $n < 500$\n",
    "\n",
    "> > > sample $M = \\{\\sqrt{n}$ points from data}\n",
    "\n",
    "> > > $r = \\text{min dist}(p, q)$, where $p, q \\in M$\n",
    "\n",
    "> > **else**\n",
    "\n",
    "> > > $r = \\frac{d * C}{2 * (k + d)}\\sqrt{d}$\n",
    "\n",
    "> **while** num_clusters > 1:\n",
    "\n",
    "> > **for** $i = 1,..,\\ell$:\n",
    "\n",
    "> > > $unary_C(x) = \\underbrace{11...11}_{x}\\underbrace{00...00}_{C-x}$\n",
    "\n",
    "> > > sample $k$ bits from $unary_C(x)$\n",
    "\n",
    "> > > **if** $x$'s cluster is not in hash table:\n",
    "\n",
    "> > > > add $x$ to hash table\n",
    "\n",
    "> > > repeat for $n$ points\n",
    "\n",
    "> > **for** $p = 1,..,n$:\n",
    "\n",
    "> > > S = {set of points that share at least one hash table with $p$}\n",
    "\n",
    "> > > Q = {$q \\in S$ s.t. $\\text{dist}(p, q) < r$}\n",
    "\n",
    "> > > merge $Q$'s clusters with $p$'s cluster\n",
    "\n",
    "> > **if** num_clusters > 1:\n",
    "\n",
    "> > > $r = A*r$\n",
    "\n",
    "> > > $k = \\frac{d * C}{2 * r}\\sqrt{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the initial version of the LSH code, with run time profiling for `build_hash_tables()` and `LSHLink()`. Note that the vast majority of the run time is spent in build_hash_tables(); the code for nearest neighbor search and merging clusters is relatively trivial. The profiling shows that of the 157 seconds spent running `LSHLink()`, 151 seconds were in `build_hash_tables()`. As such, nearly all of the optimization efforts were spent in speeding up the creation of the hash tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "from scipy.spatial.distance import pdist\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import pstats\n",
    "\n",
    "def data_extend(data, k):\n",
    "    r, c = data.shape\n",
    "    data_extend = (reduce(lambda x, y: np.vstack((x, y)),\n",
    "                          map(lambda x: data, range(k))) +\n",
    "                  np.random.randn(r*c*k).reshape(r*k, c).round(1))\n",
    "    return(data_extend)\n",
    "\n",
    "def hac(k, data):\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # start with each point in its own cluster\n",
    "    clusters = np.arange(n)\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    while unique_clusters > k:\n",
    "        min_distances = np.zeros(n)\n",
    "        min_points = np.zeros(n).astype('int')\n",
    "\n",
    "        # for each point, find min distance to point not in cluster\n",
    "        for i in range(n):\n",
    "            point = data[i,]\n",
    "            point_cluster = clusters[i]\n",
    "            distances = np.linalg.norm(point - data, axis = 1)\n",
    "            diff_cluster_points = np.where(clusters != point_cluster)[0]\n",
    "\n",
    "            min_points[i] = diff_cluster_points[\n",
    "                np.argmin(distances[diff_cluster_points])]\n",
    "            min_distances[i] = distances[min_points[i]]\n",
    "\n",
    "        # merge clusters of the two closest points\n",
    "        point1_idx = np.argmin(min_distances)\n",
    "        point1 = data[point1_idx,]\n",
    "        point2_idx = min_points[point1_idx]\n",
    "        point2 = data[point2_idx,]\n",
    "\n",
    "        point2_cluster = clusters[point2_idx]\n",
    "        clusters[\n",
    "            np.where(clusters == point2_cluster)[0]\n",
    "        ] = clusters[point1_idx]\n",
    "\n",
    "        # update number of clusters\n",
    "        unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def unary(x, C):\n",
    "    nearest_x = int(np.round(x))\n",
    "    return((np.r_[np.ones(nearest_x),\n",
    "                  np.zeros(C-nearest_x)]))\n",
    "\n",
    "def lsh_hash(point, C):\n",
    "    res = np.concatenate(list(map(lambda x: unary(x, C), point)))\n",
    "    return(res)\n",
    "\n",
    "def get_points_in_cluster(idx, clusters, data):\n",
    "    point_cluster = clusters[idx]\n",
    "    same_cluster_points_idx = np.where(\n",
    "        clusters == point_cluster\n",
    "    )[0]\n",
    "    same_cluster_points = set(\n",
    "        map(tuple, data[same_cluster_points_idx, :])\n",
    "    )\n",
    "    return same_cluster_points\n",
    "\n",
    "def get_point_indices(data, points):\n",
    "    indices = np.where((data == points[:,None]).all(-1))[1]\n",
    "    return indices\n",
    "\n",
    "def build_hash_tables(C, d, l, k, data, clusters):\n",
    "    vals = np.arange(C*d)\n",
    "    n = data.shape[0]\n",
    "    hash_tables = defaultdict(set)\n",
    "    hash_tables_reversed = defaultdict(set)\n",
    "\n",
    "    for i in range(l):\n",
    "        I = np.random.choice(vals, k, replace = False)\n",
    "\n",
    "        for j in range(n):\n",
    "            # for every point, generate hashed point\n",
    "            # and sample k bits\n",
    "            p = data[j]\n",
    "            hashed_point = lsh_hash(p, C)[I]\n",
    "            \n",
    "            # check if any other points in p's cluster are\n",
    "            # already in this hash table\n",
    "            # and only add point to hash table if no other\n",
    "            # points from its cluster are there\n",
    "            bucket = hash_tables[tuple(hashed_point)]\n",
    "            cluster_points = get_points_in_cluster(j, clusters, data)\n",
    "\n",
    "            if not cluster_points.intersection(bucket):\n",
    "                hash_tables[tuple(hashed_point)].add(tuple(p))\n",
    "                hash_tables_reversed[tuple(p)].add(tuple(hashed_point))\n",
    "    \n",
    "    return hash_tables, hash_tables_reversed\n",
    "\n",
    "def LSHLink(data, A, l, k, C = None, cutoff = 1):\n",
    "    # set default value for C if none is provided\n",
    "    if not C:\n",
    "        C = int(np.ceil(np.max(data))) + 1\n",
    "    \n",
    "    # initializations\n",
    "    n, d = data.shape\n",
    "    clusters = np.arange(n)\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    num = n - 1\n",
    "    Z = np.zeros((n - 1, 4))\n",
    "    \n",
    "    # calculate r depending on n, either:\n",
    "    # 1. min dist from a random sample of sqrt(n) points\n",
    "    # 2. formula below\n",
    "    np.random.seed(12)\n",
    "    n_samp = int(np.ceil(np.sqrt(n)))\n",
    "    samples = data[np.random.choice(\n",
    "        n, size = n_samp, replace = False), :]\n",
    "    \n",
    "    if n < 500:\n",
    "        r = np.min(pdist(samples, 'euclidean'))\n",
    "    else:\n",
    "        r = (d * C * np.sqrt(d)) / (2 * (k + d))\n",
    "    \n",
    "    np.random.seed(6)\n",
    "    while unique_clusters > cutoff:\n",
    "        # STEP 1: Generation of hash tables\n",
    "        hash_tables, hash_tables_reversed = build_hash_tables(\n",
    "            C, d, l, k, data, clusters)\n",
    "\n",
    "        # STEP 2: Nearest neighbor search for p\n",
    "        for i in range(n):\n",
    "            # get all of those hash tables that contain point p\n",
    "            p = data[i]\n",
    "            p_hashes = hash_tables_reversed[tuple(p)]\n",
    "\n",
    "            # only proceed if p is in at least one hash table\n",
    "            if hash_tables_reversed[tuple(p)]:\n",
    "\n",
    "                # find all \"similar points\" to p: points that\n",
    "                # share at least one hash table with p, and are\n",
    "                # not in the same cluster as p\n",
    "                similar_points = reduce(\n",
    "                    lambda x, y: x.union(y),\n",
    "                    map(lambda x: hash_tables[x], p_hashes)\n",
    "                    ).difference(\n",
    "                    get_points_in_cluster(i, clusters, data)\n",
    "                )\n",
    "                similar_points = np.array(list(similar_points))\n",
    "\n",
    "                # STEP 3: Connect pairs of clusters within certain\n",
    "                # distance of p; only proceed if p has any similar points\n",
    "                if similar_points.size:\n",
    "\n",
    "                    # find similar points q s.t. dist(p, q) < r\n",
    "                    # the clusters containing these points will\n",
    "                    # be merged with p's cluster\n",
    "                    points_to_merge = similar_points[\n",
    "                        np.where(np.linalg.norm(\n",
    "                            p - similar_points, axis = 1\n",
    "                        ) < r)[0]\n",
    "                    ]\n",
    "\n",
    "                    # identify which clusters contain points_to_merge\n",
    "                    clusters_to_merge = clusters[np.where(\n",
    "                        (iris == points_to_merge[:,None]).all(-1)\n",
    "                    )[1]]\n",
    "\n",
    "                    # update cluster labels\n",
    "                    clusters[np.where(\n",
    "                        np.in1d(clusters,clusters_to_merge)\n",
    "                    )[0]] = clusters[i]\n",
    "\n",
    "        # STEP 4: update parameters and continue until\n",
    "        # unique_clusters == cutoff\n",
    "        unique_clusters = len(np.unique(clusters))\n",
    "\n",
    "        #increase r and decrease k\n",
    "        r *= A\n",
    "        k = int(np.round((d * C * np.sqrt(d)) / (2 * r)))\n",
    "\n",
    "    return(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris().data\n",
    "iris = data_extend(iris, 10) * 10\n",
    "iris += np.abs(np.min(iris))\n",
    "\n",
    "l = 10\n",
    "k = 100\n",
    "n, d = iris.shape\n",
    "C = int(np.ceil(np.max(iris))) + 1\n",
    "clusters = np.arange(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 41s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 hac(1, iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 13s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file 'LSHLink.prof'. \n"
     ]
    }
   ],
   "source": [
    "%prun -q -D LSHLink.prof LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 30 14:18:31 2018    LSHLink.prof\n",
      "\n",
      "         26039956 function calls in 157.957 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 60 to 1 due to restriction <'build_hash_tables'>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        7   11.719    1.674  151.201   21.600 <ipython-input-1-b260ed9440e0>:68(build_hash_tables)\n",
      "\n",
      "\n",
      "Mon Apr 30 14:18:31 2018    LSHLink.prof\n",
      "\n",
      "         26039956 function calls in 157.957 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 60 to 1 due to restriction <'LSHLink'>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.689    0.689  157.956  157.956 <ipython-input-1-b260ed9440e0>:93(LSHLink)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats('LSHLink.prof')\n",
    "p.sort_stats('time', 'cumulative').print_stats(\n",
    "    'build_hash_tables')\n",
    "p.sort_stats('time', 'cumulative').print_stats(\n",
    "    'LSHLink')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary optimization in the second version of the code was caching the hash points using `lru_cache()` from `functools`. By storing the hashed unary values and point representations, the `build_hash_tables()` only had to compute each point's hash value on the initial run, rather than on each iteration; this was possible since a point's unary representation remains the same for a given C value, and C does not change throughout the course of the code.\n",
    "\n",
    "The other optimization was to introduce more control logic into the nearest neighbor search. Specifically, additional \"if\" statements were added, such that if no points were found within distance $r$ of point $p$, that iteration of the for loop terminates rather than continuing on to attempt to find clusters that can be merged.\n",
    "\n",
    "These changes reduced the run time from approximately 2 minutes 20 seconds to about 20 seconds. The code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from functools import reduce, lru_cache\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "def data_extend(data, k):\n",
    "    r, c = data.shape\n",
    "    data_extend = (reduce(lambda x, y: np.vstack((x, y)),\n",
    "                          map(lambda x: data, range(k))) +\n",
    "                  np.random.randn(r*c*k).reshape(r*k, c).round(1))\n",
    "    return(data_extend)\n",
    "\n",
    "def hac(k, data):\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # start with each point in its own cluster\n",
    "    clusters = np.arange(n)\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    while unique_clusters > k:\n",
    "        min_distances = np.zeros(n)\n",
    "        min_points = np.zeros(n).astype('int')\n",
    "\n",
    "        # for each point, find min distance to point not in cluster\n",
    "        for i in range(n):\n",
    "            point = data[i,]\n",
    "            point_cluster = clusters[i]\n",
    "            distances = np.linalg.norm(point - data, axis = 1)\n",
    "            diff_cluster_points = np.where(\n",
    "                clusters != point_cluster)[0]\n",
    "\n",
    "            min_points[i] = diff_cluster_points[\n",
    "                np.argmin(distances[diff_cluster_points])]\n",
    "            min_distances[i] = distances[min_points[i]]\n",
    "\n",
    "        # merge clusters of the two closest points\n",
    "        point1_idx = np.argmin(min_distances)\n",
    "        point1 = data[point1_idx,]\n",
    "        point2_idx = min_points[point1_idx]\n",
    "        point2 = data[point2_idx,]\n",
    "\n",
    "        point2_cluster = clusters[point2_idx]\n",
    "        clusters[np.where(\n",
    "            clusters == point2_cluster\n",
    "        )[0]] = clusters[point1_idx]\n",
    "\n",
    "        # update number of clusters\n",
    "        unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def unary(x, C):\n",
    "    nearest_x = int(np.round(x))\n",
    "    return(np.r_[np.ones(nearest_x), np.zeros(C-nearest_x)])\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def lsh_hash(point, C):\n",
    "    point = np.array(point)\n",
    "    res = np.concatenate(list(map(lambda x: unary(x, C), point)))\n",
    "    return(res)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_points_in_cluster(idx, clusters, data):\n",
    "    clusters = np.array(clusters)\n",
    "    data = pickle.loads(data)\n",
    "    point_cluster = clusters[idx]\n",
    "    same_cluster_points_idx = np.where(\n",
    "        clusters == point_cluster)[0]\n",
    "    same_cluster_points = set(\n",
    "        map(tuple, data[same_cluster_points_idx, :])\n",
    "    )\n",
    "    return(same_cluster_points)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_point_indices(data, points):\n",
    "    data = pickle.loads(data)\n",
    "    points = pickle.loads(points)\n",
    "    indices = np.where((data == points[:,None]).all(-1))[1]\n",
    "    return(indices)\n",
    "\n",
    "def build_hash_tables(C, d, l, k, data, clusters):\n",
    "    vals = np.arange(C*d)\n",
    "    n = data.shape[0]\n",
    "    hash_tables = defaultdict(set)\n",
    "    hash_tables_reversed = defaultdict(set)\n",
    "\n",
    "    for i in range(l):\n",
    "        I = np.random.choice(vals, k, replace = False)\n",
    "\n",
    "        for j in range(n):\n",
    "            # for every point, generate hashed point\n",
    "            # and sample k bits\n",
    "            p = data[j]\n",
    "            hashed_point = lsh_hash(tuple(p), C)[I]\n",
    "            \n",
    "            # check if any other points in p's cluster are\n",
    "            # already in this hash table and only add point to\n",
    "            # hash table if no other points from its cluster are there\n",
    "            bucket = hash_tables[tuple(hashed_point)]\n",
    "            cluster_points = get_points_in_cluster(\n",
    "                j, tuple(clusters), pickle.dumps(data)\n",
    "            )\n",
    "\n",
    "            if not cluster_points.intersection(bucket):\n",
    "                hash_tables[tuple(hashed_point)].add(tuple(p))\n",
    "                hash_tables_reversed[tuple(p)].add(tuple(hashed_point))\n",
    "    \n",
    "    return hash_tables, hash_tables_reversed\n",
    "\n",
    "def LSHLink(data, A, l, k, C = None, cutoff = 1, dendrogram = False):\n",
    "    # set default value for C if none is provided\n",
    "    if not C:\n",
    "        C = int(np.ceil(np.max(data))) + 1\n",
    "    \n",
    "    if dendrogram and cutoff != 1:\n",
    "        raise Exception(\n",
    "            'Dendrogram requires a full hierarchy; set cutoff to 1'\n",
    "        )\n",
    "    \n",
    "    # initializations\n",
    "    n, d = data.shape\n",
    "    clusters = np.arange(n)\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    num = n - 1\n",
    "    Z = np.zeros((n - 1, 4))\n",
    "    \n",
    "    # calculate r depending on n, either:\n",
    "    # 1. min dist from a random sample of sqrt(n) points\n",
    "    # 2. formula below\n",
    "    np.random.seed(12)\n",
    "    n_samp = int(np.ceil(np.sqrt(n)))\n",
    "    samples = data[\n",
    "        np.random.choice(n, size = n_samp, replace = False), :]\n",
    "    \n",
    "    if n < 500:\n",
    "        r = np.min(pdist(samples, 'euclidean'))\n",
    "    else:\n",
    "        r = (d * C * np.sqrt(d)) / (2 * (k + d))\n",
    "    \n",
    "    np.random.seed(6)\n",
    "\n",
    "    while unique_clusters > cutoff:\n",
    "        # STEP 1: Generation of hash tables\n",
    "        hash_tables, hash_tables_reversed = build_hash_tables(\n",
    "            C, d, l, k, data, clusters)\n",
    "\n",
    "        # STEP 2: Nearest neighbor search for p\n",
    "        for i in range(n):\n",
    "            # get all of those hash tables that contain point p\n",
    "            p = data[i]\n",
    "            p_hashes = hash_tables_reversed[tuple(p)]\n",
    "\n",
    "            # only proceed if p is in at least one hash table\n",
    "            if hash_tables_reversed[tuple(p)]:\n",
    "\n",
    "                # find all \"similar points\" to p: points that share\n",
    "                # at least one hash table with p, and are not in the\n",
    "                # same cluster as p\n",
    "                similar_points = reduce(\n",
    "                    lambda x, y: x.union(y),\n",
    "                    map(lambda x: hash_tables[x], p_hashes)\n",
    "                    ).difference(\n",
    "                    get_points_in_cluster(i, tuple(clusters),\n",
    "                                          pickle.dumps(data))\n",
    "                )\n",
    "                similar_points = np.array(list(similar_points))\n",
    "\n",
    "                # STEP 3: Connect pairs of clusters within certain\n",
    "                # distance of p\n",
    "                # only proceed if p has any similar points\n",
    "                if similar_points.size:\n",
    "\n",
    "                    # find similar points q s.t. dist(p, q) < r\n",
    "                    # the clusters containing these points will be\n",
    "                    # merged with p's cluster\n",
    "                    points_to_merge = similar_points[\n",
    "                        np.where(\n",
    "                            np.linalg.norm(p - similar_points, axis = 1) < r\n",
    "                        )[0]\n",
    "                    ]\n",
    "\n",
    "                    # only proceed if p has similar points within distance r\n",
    "                    if points_to_merge.size:\n",
    "                        # identify which clusters contain points_to_merge\n",
    "                        point_indices = get_point_indices(\n",
    "                            pickle.dumps(data),\n",
    "                            pickle.dumps(points_to_merge)\n",
    "                        )\n",
    "                        clusters_to_merge = list(\n",
    "                            np.unique(clusters[point_indices])\n",
    "                        )\n",
    "                        \n",
    "                        # update cluster labels\n",
    "                        # if dendrogram = False, we can use a simpler method\n",
    "                        if not dendrogram:\n",
    "                            clusters[\n",
    "                                np.where(np.in1d(clusters, clusters_to_merge)\n",
    "                                        )[0]] = clusters[i]\n",
    "                        \n",
    "                        else:\n",
    "                            clusters_to_merge.append(clusters[i])\n",
    "                            \n",
    "                            for j in range(len(clusters_to_merge) - 1):\n",
    "                                clusterA = clusters_to_merge[j]\n",
    "                                clusterB = clusters_to_merge[j+1]\n",
    "                                num += 1\n",
    "                                clusters[np.where(\n",
    "                                    np.in1d(clusters, [clusterA, clusterB])\n",
    "                                )[0]] = num\n",
    "\n",
    "                                Z[num - n, :] = np.array(\n",
    "                                    [clusterA,\n",
    "                                     clusterB,\n",
    "                                     r,\n",
    "                                     len(np.where(np.in1d(clusters, num))[0])]\n",
    "                                )\n",
    "                                clusters_to_merge[j:j+2] = 2 * [num]\n",
    "\n",
    "        # STEP 4: update parameters and continue until\n",
    "        # unique_clusters == cutoff\n",
    "        unique_clusters = len(np.unique(clusters))\n",
    "\n",
    "        #increase r and decrease k\n",
    "        r *= A\n",
    "        k = int(np.round((d * C * np.sqrt(d)) / (2 * r)))\n",
    "\n",
    "    if not dendrogram:\n",
    "        return(clusters)\n",
    "    \n",
    "    else:\n",
    "        return(clusters, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris().data\n",
    "iris = data_extend(iris, 10) * 10\n",
    "iris += np.abs(np.min(iris))\n",
    "\n",
    "l = 10\n",
    "k = 100\n",
    "n, d = iris.shape\n",
    "C = int(np.ceil(np.max(iris))) + 1\n",
    "clusters = np.arange(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 LSHLink(iris, A = 1.4, l = 10, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
